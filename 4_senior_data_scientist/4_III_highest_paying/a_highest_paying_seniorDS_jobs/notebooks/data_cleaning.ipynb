{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1686 entries, 0 to 1685\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   job_title           1686 non-null   object \n",
      " 1   job_location        1661 non-null   object \n",
      " 2   job_schedule_type   1685 non-null   object \n",
      " 3   job_work_from_home  335 non-null    float64\n",
      " 4   company_name        1686 non-null   object \n",
      " 5   salary_year_avg     1686 non-null   float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 79.2+ KB\n",
      "None\n",
      "       job_work_from_home  salary_year_avg\n",
      "count               335.0      1686.000000\n",
      "mean                  1.0    154050.031829\n",
      "std                   0.0     50865.050688\n",
      "min                   1.0     45000.000000\n",
      "25%                   1.0    125000.000000\n",
      "50%                   1.0    155000.000000\n",
      "75%                   1.0    173000.000000\n",
      "max                   1.0    890000.000000\n",
      "\n",
      "\n",
      "job_title           0\n",
      "job_location       25\n",
      "company_name        0\n",
      "salary_year_avg     0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values in job_title:\n",
      "job_title\n",
      "Senior Data Scientist                                             578\n",
      "Senior Data Scientist (Remote)                                     23\n",
      "Senior Manager, Data Science                                       16\n",
      "Senior Data Scientist PD23R103 - Bureau of Health Workforce        14\n",
      "Data Scientist, Senior                                             14\n",
      "                                                                 ... \n",
      "Sr Data Scientist - Marketing Science (Greater NYC Area, NY)        1\n",
      "Senior Data Scientist , Search and Personalization (14498) ...      1\n",
      "Senior Data Scientist , Search and Personalization (15258) ...      1\n",
      "Senior Data Scientist, Music                                        1\n",
      "Senior Data Scientist/Engineer (NLP/NLU)                            1\n",
      "Name: count, Length: 803, dtype: int64\n",
      "\n",
      "Unique Values in job_location:\n",
      "job_location\n",
      "Anywhere                  335\n",
      "New York, NY               79\n",
      "United States              73\n",
      "San Francisco, CA          67\n",
      "Atlanta, GA                34\n",
      "                         ... \n",
      "Germany                     1\n",
      "Sofia, Bulgaria             1\n",
      "Amsterdam, Netherlands      1\n",
      "Carlsbad, CA                1\n",
      "Philippines                 1\n",
      "Name: count, Length: 397, dtype: int64\n",
      "\n",
      "Unique Values in company_name:\n",
      "company_name\n",
      "Harnham                66\n",
      "Walmart                25\n",
      "Booz Allen Hamilton    24\n",
      "CVS Health             23\n",
      "TikTok                 22\n",
      "                       ..\n",
      "La Javaness             1\n",
      "EMW, Inc.               1\n",
      "Trainline               1\n",
      "HoneyBook               1\n",
      "Rollio.ai               1\n",
      "Name: count, Length: 895, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Initial Data Exploration\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = '../data/highest_paying_seniorDS_jobs.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display Basic Information\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Remove the 'job_schedule_type' & 'job_work_from_home' (Won't be using them)\n",
    "df.drop(['job_schedule_type','job_work_from_home'], axis=1,inplace=True)\n",
    "\n",
    "# Check for Missing Values\n",
    "print('\\n')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display unique values in categorical columns (check for any unexpected categories)\n",
    "for col_name in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\nUnique Values in {col_name}:\")\n",
    "    print(df[col_name].value_counts()) # print the unique values and their counts (categorical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminary Remarks \n",
    "- salary_year_avg of float64 dtype hence, no data type conversion required. Also, all values are in standardized format (no currency symbols or thousands separators).\n",
    "- `.info()` shows that the 'job_location' column has some null values (25 entries).\n",
    "- `.describe()` shows that the mean and median are almost equal and that the IQR and std_dev are almost equal as well.\n",
    "    - Normal Distribution: Mode=Median=Mean (Symmetry) & IQR = 1.35*std\n",
    "- Both the min and max values of the dataset clearly indicate that there are outliers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         job_title job_location  \\\n",
      "4                            Senior Data Scientist          NaN   \n",
      "8                            Senior Data Scientist          NaN   \n",
      "26                           Senior Data Scientist          NaN   \n",
      "51  Senior Data Scientist | Hedge Fund | $200-300k          NaN   \n",
      "\n",
      "          company_name  salary_year_avg  \n",
      "4   Algo Capital Group         375000.0  \n",
      "8   Algo Capital Group         375000.0  \n",
      "26  Algo Capital Group         325000.0  \n",
      "51               Orbis         250000.0  \n",
      "\n",
      "After replacing null values:\n",
      "                                         job_title job_location  \\\n",
      "4                            Senior Data Scientist      Unknown   \n",
      "8                            Senior Data Scientist      Unknown   \n",
      "26                           Senior Data Scientist      Unknown   \n",
      "51  Senior Data Scientist | Hedge Fund | $200-300k      Unknown   \n",
      "\n",
      "          company_name  salary_year_avg  \n",
      "4   Algo Capital Group         375000.0  \n",
      "8   Algo Capital Group         375000.0  \n",
      "26  Algo Capital Group         325000.0  \n",
      "51               Orbis         250000.0  \n"
     ]
    }
   ],
   "source": [
    "# 2. Handling Missing Values\n",
    "\n",
    "# Get rows with any null values\n",
    "rows_with_null = df[df.isnull().any(axis=1)] # Returns a DataFrame containing only the rows where at least one column has a null value\n",
    "rows_with_null_index = rows_with_null.index # To display the rows that previously had null values after they have been replaced\n",
    "print(rows_with_null.head(4))\n",
    "# OR Check for null values in 'job_location'\n",
    "# rows_with_null_in_column = df[df['job_location'].isnull()]\n",
    "\n",
    "# Replace NaN values in the 'job_location' column with 'Unknown' to ensure that, when grouping by multiple columns (including \n",
    "# 'job_location'), these NaN values won't interfere with the groupby operation, i.e., they won't be treated as unique values.\n",
    "df.loc[:, 'job_location'] = df['job_location'].fillna('Unknown') # Replace NaN with 'Unknown' only in 'job_location' column (Modifies the original DataFrame)\n",
    "# Display the updated rows that previously had null values\n",
    "print(\"\\nAfter replacing null values:\")\n",
    "print(df.loc[rows_with_null_index].head(4)) # The loc accessor is used to select rows by their index labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we are primarily interested in the salary_year_avg column, it is not necessary to remove or fill the NaN values of the 'job_location' column. \n",
    "- There are some duplicate rows in the preceding DataFrame. Duplicate rows will be removed in the next data cleaning step. \n",
    "    - The presence of the 25 NaN values in the 'job_location' column will not prevent these rows from being identified as duplicates and removed (except for the first occurrence), due to the way the drop_duplicates() method treats NaN values (they are treated as equal to each other).\n",
    "    - However, these NaN values would interfere with the groupby operations in the next code cell, as NaN values are treated as unique in such operations. To address this, the NaN values in the 'job_location' column were replaced with 'Unknown'. This replacement allows the three duplicate rows that previously contained NaN values to be grouped together with their identical counterparts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of duplicates (without the first occurrence) is 96.\n",
      "Hence, the number of remaining entries will be equal to 1590.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary_year_avg</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>157500.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>170000.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>160000.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115000.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>175000.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91800.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    salary_year_avg  count\n",
       "26         157500.0     11\n",
       "31         170000.0      9\n",
       "27         160000.0      7\n",
       "6          115000.0      6\n",
       "33         175000.0      5\n",
       "2           91800.0      3"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 3. Identifying Duplicates\n",
    "\n",
    "# Identify duplicate rows based on the specified columns (Mark all duplicates as 'True', except for the first occurrence)\n",
    "duplicate_rows = df[df.duplicated(subset=['job_title', 'job_location', 'company_name', 'salary_year_avg'], keep = 'first')]\n",
    "\n",
    "# Calculate the total number of rows that will be removed and the number of remaining entries\n",
    "entries = 1686 # total number of entries\n",
    "number_of_duplicates = duplicate_rows['salary_year_avg'].count()\n",
    "print(f\"The total number of duplicates (without the first occurrence) is {number_of_duplicates}.\")\n",
    "print(f\"Hence, the number of remaining entries will be equal to {entries-number_of_duplicates}.\")\n",
    "\n",
    "# Group the duplicate rows by the specified columns, find the count of rows in each group, \n",
    "# and integrate the count of each group into the resulting DataFrame.\n",
    "duplicate_counts = (\n",
    "    duplicate_rows.groupby(by=['job_title', 'job_location', 'company_name', 'salary_year_avg'])\n",
    "    .size() # size() to count all items in each group, including NaN values.\n",
    "    .reset_index(name='count') # Convert back into Dataframe, reset index, and integrate the count into the DataFrame as a new column named 'count'.\n",
    ")\n",
    "\n",
    "# Sort based on the count of rows in each group\n",
    "sorted_duplicates = duplicate_counts.sort_values(by='count', ascending=False)\n",
    "# Save the DataFrame to a CSV\n",
    "sorted_duplicates.to_csv('../data/sorted_duplicates.csv',index=False)\n",
    "\n",
    "# Group by unique salary values to identify potential duplicate clusters across different salaries, which could skew the distribution.\n",
    "# Use .sum() to get the total number of duplicate rows for each unique salary value (except for the first occurrences)\n",
    "clustered_duplicates = (\n",
    "    sorted_duplicates.groupby('salary_year_avg')\n",
    "    ['count'].sum() # Returns a Series where the index corresponds to the unique salary values and the values represent the total number of duplicate rows for each unique salary value\n",
    "    .reset_index() # Converts the Series back into a DataFrame and resets its index (The index of the Series becomes the 'salary_year_avg' column)\n",
    "    .sort_values(by=['count','salary_year_avg'], ascending=False)    \n",
    ")\n",
    "clustered_duplicates.to_csv('../data/clustered_duplicates.csv',index=False)\n",
    "clustered_duplicates.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       salary_range  count\n",
      "9   $170000-$179999     17\n",
      "7   $150000-$159999     17\n",
      "3   $110000-$119999     13\n",
      "8   $160000-$169999     11\n",
      "4   $120000-$129999      7\n",
      "5   $130000-$139999      6\n",
      "12  $200000-$209999      4\n",
      "11  $190000-$199999      3\n",
      "6   $140000-$149999      3\n",
      "2     $90000-$99999      3\n",
      "14  $250000-$259999      2\n",
      "13  $210000-$219999      2\n",
      "10  $180000-$189999      2\n",
      "17  $340000-$349999      1\n",
      "16  $320000-$329999      1\n",
      "15  $310000-$319999      1\n",
      "1     $70000-$79999      1\n",
      "0     $50000-$59999      1\n"
     ]
    }
   ],
   "source": [
    "# Group the salary values into ranges of $10,000\n",
    "\n",
    "# Create bins for salary ranges of $10,000\n",
    "bins = range(50000, 360000, 10000)\n",
    "labels = [f'${i}-${i+9999}' for i in bins[:-1]] # Generates labels for each bin (e.g., '$50000-$59999', '$60000-$69999', etc.). \n",
    "# Add a new column for the salary ranges (Use pd.cut() to categorize the 'salary_year_avg' values into the defined ranges.)\n",
    "clustered_duplicates['salary_range'] = pd.cut(clustered_duplicates['salary_year_avg'], bins=bins, labels=labels, right=False)\n",
    "# Group by salary_range, sum the counts, reset index, and sort\n",
    "salary_range_sum = (\n",
    "    clustered_duplicates.groupby('salary_range', observed=True) # Includes only observed categories\n",
    "    ['count'].sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=['count','salary_range'], ascending=False)\n",
    ") \n",
    "print(salary_range_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove the duplicate rows while keeping the first occurrence\n",
    "cleaned_df = (\n",
    "    df.drop_duplicates(subset=['job_title', 'job_location', 'company_name', 'salary_year_avg'], keep='first')\n",
    "    .copy() # This ensures that cleaned_df is a new DataFrame, not a view of the original one.\n",
    "    .reset_index(drop=True) # This ensures that the index is reset to a continuous range after removing duplicates.\n",
    ")\n",
    "# 4. Validate Data integrity\n",
    "assert all(cleaned_df['salary_year_avg'] > 0) , \"There are negative salaries!\"\n",
    "\n",
    "# 5. Create Boolean Indicator Column (Used in 4_III.b to extract the highest-paying jobs)\n",
    "median_salary = cleaned_df['salary_year_avg'].median() # Calculate the median of the distribution\n",
    "# Use the .loc accessor to safely assign the new column (Ensures that the original DataFrame is modified directly, rather than potentially working on a copy)\n",
    "cleaned_df.loc[:,'above_median'] = (cleaned_df['salary_year_avg'] >= median_salary) # Select all rows in the 'above_median' column (which will be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1590 entries, 0 to 1589\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   job_title        1590 non-null   object \n",
      " 1   job_location     1590 non-null   object \n",
      " 2   company_name     1590 non-null   object \n",
      " 3   salary_year_avg  1590 non-null   float64\n",
      " 4   above_median     1590 non-null   bool   \n",
      "dtypes: bool(1), float64(1), object(3)\n",
      "memory usage: 51.4+ KB\n",
      "None\n",
      "       salary_year_avg\n",
      "count      1590.000000\n",
      "mean     153653.360481\n",
      "std       50855.750194\n",
      "min       45000.000000\n",
      "25%      125000.000000\n",
      "50%      155000.000000\n",
      "75%      173000.000000\n",
      "max      890000.000000\n",
      "\n",
      "\n",
      "job_title          0\n",
      "job_location       0\n",
      "company_name       0\n",
      "salary_year_avg    0\n",
      "above_median       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 6. Final Data Quality Check\n",
    "print(cleaned_df.info())\n",
    "print(cleaned_df.describe())\n",
    "print('\\n')\n",
    "print(cleaned_df.isnull().sum())\n",
    "\n",
    "# 7. Document Changes\n",
    "cleaning_log = [\n",
    "    \"1. Handled missing values (NaN to 'Unknown')\",\n",
    "    \"2. Removed duplicates\",\n",
    "    \"3. Validated data integrity (salaries > 0)\",\n",
    "    \"4. Created Boolean Indicator Column (salaries > median)\"\n",
    "]\n",
    "with open('../data/data_cleaning_log.txt', 'w') as f: # Opens the file in write mode and closes it after the loop ends\n",
    "    for step in cleaning_log:  # Loop that iterates over each item in the 'cleaning_log' list\n",
    "        f.write(f\"{step}\\n\")   # Write each step (string) to the file\n",
    "\n",
    "# 8. Save Cleaned Data\n",
    "cleaned_df.to_csv('../data/cleaned_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the duplicate rows were dispersed around the median of the distribution (Q2=$155,000) and thus, were skewing the distribution in the vicinity of the median. A total number of 96 duplicate rows were removed from the original dataset that used to comprise 1686 entries. These duplicates accounted for a 5.7% error in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These duplicate rows are definitely a limitation of the current project, and they should be analyzed separately in a future project. That is, it should be determined whether the few duplicates are due to multiple identical job postings being posted on the same day or within a short time frame, or due to the same job opening being posted throughout the year at regular intervals to attract multiple candidates.\n",
    "To distinguish between these two cases, filtering based on the date column (job_posted_date) has to occur.\n",
    "- If the same job posting reappears every 2-4 months, keep it.\n",
    "- If the same job posting reappears on the same day, days after the original post, or even a couple weeks after it, keep only the original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
